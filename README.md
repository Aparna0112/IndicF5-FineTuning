# IndicF5 Malayalam Fine-tuning

Fine-tune the IndicF5 Text-to-Speech model for Malayalam language using the MALAYALAM_STT_COMBINED_1_2 dataset on RunPod.

## ğŸš€ Quick Start on RunPod

### 1. Launch RunPod Instance
- **GPU**: RTX 4090, A100, or H100 (recommended: A100 40GB)
- **Template**: PyTorch 2.1+ template
- **Storage**: At least 50GB

### 2. One-Command Setup
```bash
# Clone repository
git clone https://github.com/Aparna0112/IndicF5-FineTuning.git
cd IndicF5-FineTuning

# Quick setup
chmod +x *.sh
./quick_setup.sh
```

### 3. Prepare Dataset
```bash
python3 prepare_dataset.py
```

### 4. Start Training
```bash
./start_training.sh
```

### 5. Monitor Training
```bash
# In another terminal
./monitor.sh
```

## ğŸ“ Project Structure

```
IndicF5-FineTuning/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.py                 # Training configuration
â”œâ”€â”€ dataset.py               # Malayalam TTS dataset class
â”œâ”€â”€ train.py                 # Main training script
â”œâ”€â”€ prepare_dataset.py       # Dataset preparation
â”œâ”€â”€ inference.py             # Text-to-speech inference
â”œâ”€â”€ setup_runpod.py          # Environment setup
â”œâ”€â”€ start_training.sh        # Training launcher
â”œâ”€â”€ monitor.sh              # System monitor
â”œâ”€â”€ quick_setup.sh          # One-command setup
â””â”€â”€ data/
    â”œâ”€â”€ audio/              # Raw audio files
    â”œâ”€â”€ processed/          # Processed dataset
    â””â”€â”€ dataset_stats.json  # Dataset statistics
```

## ğŸ”§ Configuration

Key settings in `config.py`:

```python
class Config:
    model_name = "ai4bharat/IndicF5"
    language = "malayalam"
    batch_size = 8  # Auto-adjusted based on GPU memory
    learning_rate = 5e-5
    num_epochs = 15
    max_audio_length = 8.0  # seconds
    use_wandb = True
    wandb_project = "indicf5-malayalam-runpod"
```

## ğŸ“Š Dataset

- **Source**: `ceymox/MALAYALAM_STT_COMBINED_1_2` from HuggingFace
- **Language**: Malayalam
- **Size**: ~3GB download, varies after processing
- **Quality Filters**: 
  - Audio duration: 1-10 seconds
  - Text length: 5-200 characters
  - Train/Test split: 90/10

## ğŸƒâ€â™‚ï¸ Training

### Automatic Training
```bash
./start_training.sh
```

### Manual Training
```bash
# Setup environment
python3 setup_runpod.py

# Prepare data
python3 prepare_dataset.py

# Train model
python3 train.py
```

### Training Features
- âœ… Mixed precision training (FP16)
- âœ… Gradient accumulation
- âœ… Automatic checkpoint management
- âœ… WandB logging
- âœ… GPU memory optimization
- âœ… Error handling and recovery

## ğŸ“ˆ Monitoring

### Real-time Monitoring
```bash
# System monitor
./monitor.sh

# GPU usage
watch -n 1 nvidia-smi

# Training logs
tail -f logs/training.log
```

### WandB Dashboard
- Project: `indicf5-malayalam-runpod`
- Metrics: loss, learning rate, epoch progress
- Login: `wandb login` (optional)

## ğŸ¯ Inference

### Test Trained Model
```bash
python3 inference.py
```

### Custom Inference
```python
from inference import load_finetuned_model, generate_speech, save_audio

# Load model
model = load_finetuned_model()

# Generate speech
malayalam_text = "à´®à´²à´¯à´¾à´³à´‚ à´Ÿàµ†à´•àµà´¸àµà´±àµà´±àµ à´Ÿàµ à´¸àµà´ªàµ€à´šàµà´šàµ"
audio = generate_speech(model, malayalam_text)

# Save audio
save_audio(audio, "output.wav")
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

**CUDA Out of Memory**
```bash
# Reduce batch size in config.py
batch_size = 2  # or 4
```

**Dataset Download Fails**
```bash
# Manual retry
python3 prepare_dataset.py
```

**Import Errors**
```bash
# Reinstall dependencies
pip install -r requirements.txt
```

**Model Loading Issues**
```bash
# Check model name and HuggingFace access
# Verify: ai4bharat/IndicF5
```

### Performance Tips
- Use A100 40GB for best performance
- Monitor GPU memory with `nvidia-smi`
- Use tmux for long training sessions
- Download checkpoints regularly

## ğŸ’¾ Checkpoints

Checkpoints are saved to `./checkpoints/indicf5_malayalam/`:
- `best_model.pt` - Best validation loss
- `checkpoint_epoch_N.pt` - Epoch checkpoints
- Automatic cleanup keeps latest 3 checkpoints

### Download Checkpoints
```bash
# Compress before download
tar -czf checkpoints.tar.gz checkpoints/

# Download to local machine
# scp root@runpod-ip:/workspace/IndicF5-FineTuning/checkpoints.tar.gz ./
```

## ğŸ“ Requirements

- Python 3.9+
- PyTorch 2.0+
- CUDA 12.1+ (recommended)
- 16GB+ GPU memory (minimum 8GB)
- 50GB+ storage

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test on RunPod
5. Submit a pull request

## ğŸ“„ License

This project is open source. Please respect the licenses of:
- IndicF5 model (AI4Bharat)
- Dataset (ceymox/MALAYALAM_STT_COMBINED_1_2)

## ğŸ†˜ Support

- **Issues**: GitHub Issues
- **Dataset**: [HuggingFace Dataset](https://huggingface.co/datasets/ceymox/MALAYALAM_STT_COMBINED_1_2)
- **Model**: [IndicF5](https://huggingface.co/ai4bharat/IndicF5)

## ğŸ† Results

After training, you should have:
- Fine-tuned IndicF5 model for Malayalam TTS
- Checkpoints for inference
- Training logs and metrics
- Generated speech samples

---

**Happy Training! ğŸš€**
